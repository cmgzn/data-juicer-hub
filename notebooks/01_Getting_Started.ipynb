{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chapter 1: Getting Started\n",
                "\n",
                "**Data-Juicer User Guide**\n",
                "\n",
                "- Git Commit: `v1.4.5`\n",
                "- Commit Date: 2026-01-16\n",
                "- Repository: https://github.com/datajuicer/data-juicer\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Table of Contents\n",
                "\n",
                "1. [Install Data-Juicer](#install-data-juicer)\n",
                "2. [Create Sample JSONL Data](#create-sample-jsonl-data)\n",
                "3. [Write Basic YAML Config](#write-basic-yaml-config)\n",
                "4. [Execute Pipeline](#execute-pipeline)\n",
                "5. [Check Output](#check-output)\n",
                "6. [Learning Path](#learning-path)\n",
                "   - [Core Concepts (Recommended Order)](#core-concepts-recommended-order)\n",
                "   - [Advanced Topics (Appendices)](#advanced-topics-appendices)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install Data-Juicer\n",
                "\n",
                "Data-Juicer can be easily installed via pip. We recommend using `uv` for faster installation, but standard `pip` works too.\n",
                "\n",
                "Detailed installation tutorial [here](https://datajuicer.github.io/data-juicer/en/main/docs/tutorial/Installation.html)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If running in Google Colab, use 'pip install' instead of 'uv pip install'\n",
                "!uv pip install py-data-juicer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Sample JSONL Data\n",
                "\n",
                "Data-Juicer works with JSONL (JSON Lines) format, where each line is a valid JSON object. This format is efficient for streaming large datasets and is widely used in the ML community."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "# Create data directory\n",
                "os.makedirs('./data', exist_ok=True)\n",
                "\n",
                "# Sample data\n",
                "samples = [\n",
                "    {\"text\": \"Today is Sunday and it's a happy day!\", \"meta\": {\"src\": \"web\", \"date\": \"2024-01-01\"}},\n",
                "    {\"text\": \"Do you need a cup of coffee?\", \"meta\": {\"src\": \"social\", \"author\": \"user123\"}},\n",
                "    {\"text\": \"Machine learning is transforming the world.\", \"meta\": {\"src\": \"article\"}},\n",
                "    {\"text\": \"Short.\", \"meta\": {\"src\": \"web\"}},\n",
                "    {\"text\": \"This is a longer text with more content to demonstrate filtering capabilities.\", \"meta\": {\"src\": \"blog\"}}\n",
                "]\n",
                "\n",
                "# Write JSONL file\n",
                "with open('./data/sample.jsonl', 'w') as f:\n",
                "    for sample in samples:\n",
                "        f.write(json.dumps(sample) + '\\n')\n",
                "\n",
                "print(f\"Created sample dataset with {len(samples)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Write Basic YAML Config\n",
                "\n",
                "Data-Juicer uses YAML configuration files (\"recipes\") to define processing pipelines. A recipe specifies:\n",
                "- **Input/Output paths**: Where to read and write data\n",
                "- **Processing operators**: What transformations to apply\n",
                "- **Execution settings**: Parallelism, caching, etc.\n",
                "\n",
                "Let's create a simple recipe that filters text by length and language, then removes duplicates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import yaml\n",
                "\n",
                "# Create config as Python dictionary\n",
                "config_dict = {\n",
                "    'project_name': 'getting_started',\n",
                "    \n",
                "    # Input/Output paths\n",
                "    'dataset_path': './data/sample.jsonl',\n",
                "    'export_path': './outputs/processed.jsonl',\n",
                "    \n",
                "    # Number of parallel processes\n",
                "    'np': 1,\n",
                "    \n",
                "    # Processing pipeline\n",
                "    'process': [\n",
                "        # 1. Filter by text length\n",
                "        {\n",
                "            'text_length_filter': {\n",
                "                'min_len': 10,\n",
                "                'max_len': 200\n",
                "            }\n",
                "        },\n",
                "        # 2. Filter by language (English)\n",
                "        # Learn more: https://datajuicer.github.io/data-juicer/en/main/docs/operators/filter/language_id_score_filter.html\n",
                "        {\n",
                "            'language_id_score_filter': {\n",
                "                'lang': 'en',\n",
                "                'min_score': 0.8\n",
                "            }\n",
                "        },\n",
                "        # 3. Remove duplicates\n",
                "        {\n",
                "            'document_deduplicator': {\n",
                "                'lowercase': True\n",
                "            }\n",
                "        }\n",
                "    ]\n",
                "}\n",
                "\n",
                "# Save config dict to YAML file\n",
                "os.makedirs('./configs', exist_ok=True)\n",
                "with open('./configs/basic.yaml', 'w') as f:\n",
                "    yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
                "\n",
                "print(\"Config saved to ./configs/basic.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Execute Pipeline\n",
                "\n",
                "Data-Juicer provides two ways to run pipelines:\n",
                "1. **Command-line**: Using the `dj-process` command\n",
                "2. **Programmatic**: Using Python API for more control\n",
                "\n",
                "Both methods produce identical results. Choose based on your workflow preference.\n",
                "\n",
                "### Option 1: Command-line Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!dj-process --config ./configs/basic.yaml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Check Command-line Output\n",
                "\n",
                "Let's verify the results from the command-line execution:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Read processed data from YAML config execution\n",
                "with open('./outputs/processed.jsonl', 'r') as f:\n",
                "    processed = [json.loads(line) for line in f]\n",
                "\n",
                "print(f\"Original samples: 5\")\n",
                "print(f\"Processed samples: {len(processed)}\")\n",
                "print(\"\\nProcessed data:\")\n",
                "for i, sample in enumerate(processed, 1):\n",
                "    print(f\"\\n{i}. {sample['text']}\")\n",
                "    print(f\"   Metadata: {sample.get('meta', {})}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice how the pipeline filtered out:\n",
                "- Short texts (< 10 characters) - e.g., \"Short.\"\n",
                "- Non-English texts (language confidence < 0.8)\n",
                "- Duplicate entries\n",
                "\n",
                "### Option 2: Programmatic Execution\n",
                "\n",
                "Alternatively, you can run the pipeline entirely in Python **without any YAML file**. This approach directly uses Data-Juicer's low-level APIs for maximum flexibility:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_juicer.ops import load_ops\n",
                "from data_juicer.core.exporter import Exporter\n",
                "from data_juicer.core.data import NestedDataset\n",
                "\n",
                "# Step 1: Load dataset directly from samples list\n",
                "ds = NestedDataset.from_list(samples)\n",
                "print(f\"Loaded {len(ds)} samples\")\n",
                "\n",
                "# or from a JSONL file\n",
                "# from jsonargparse import Namespace\n",
                "# from data_juicer.core.data.dataset_builder import DatasetBuilder\n",
                "\n",
                "# cfg = Namespace({\"dataset_path\": config_dict[\"dataset_path\"]})\n",
                "\n",
                "# builder = DatasetBuilder(cfg)\n",
                "# ds = builder.load_dataset()\n",
                "\n",
                "# Step 2: Define operators as Python list\n",
                "process_list = config_dict[\"process\"]\n",
                "\n",
                "# Step 3: Load operators from the process list\n",
                "ops = load_ops(process_list)\n",
                "print(f\"Loaded {len(ops)} operators: {[op._name for op in ops]}\")\n",
                "\n",
                "# Step 4: Process dataset through each operator\n",
                "for op in ops:\n",
                "    ds = op.run(ds)\n",
                "    print(f\"After {op._name}: {len(ds)} samples remaining\")\n",
                "\n",
                "# Step 5: Export results\n",
                "exporter = Exporter(\"./outputs/processed_programmatic.jsonl\")\n",
                "exporter.export(ds)\n",
                "\n",
                "# Display results\n",
                "print(f\"Original samples: 5\")\n",
                "print(f\"Processed samples: {len(processed)}\")\n",
                "for i, sample in enumerate(ds):\n",
                "    print(f\"{i+1}. {sample['text']}\")\n",
                "    print(f\"   Metadata: {sample.get('meta', {})}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!cat ./outputs/processed_programmatic.jsonl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both execution methods produce the same filtered dataset:\n",
                "- **Option 1 (Command-line)**: Simple and quick for one-off processing with YAML configs\n",
                "- **Option 2 (Programmatic)**: Pure Python API without any YAML files - ideal for:\n",
                "  - Integration into larger Python workflows\n",
                "  - Fine-grained control over each processing step\n",
                "  - Debugging and step-by-step inspection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Learning Path\n",
                "\n",
                "Here's a recommended learning path through the remaining chapters:\n",
                "\n",
                "### Core Concepts (Recommended Order)\n",
                "\n",
                "1. **[Chapter 2: Building Recipes](./02_Building_Recipes.ipynb)**\n",
                "   - Understand recipe structure (global parameters, process pipeline, operator parameters)\n",
                "   - Create basic and custom recipes\n",
                "   - Override parameters via CLI\n",
                "   - Explore pre-defined recipes from the Recipe Gallery (data-juicer-hub)\n",
                "\n",
                "2. **[Chapter 3: Data Formats and Loading](./03_Data_Formats_and_Loading.ipynb)**\n",
                "   - Learn Data-Juicer's unified format (DJ Format)\n",
                "   - Convert between dialog formats (Messages, ShareGPT, Alpaca, Query-Response)\n",
                "   - Handle multimodal format conversion (LLaVA, MMC4, InternVid, etc.)\n",
                "\n",
                "3. **[Chapter 4: DJ Dataset API](./04_DJ_Dataset_API.ipynb)**\n",
                "   - Use NestedDataset (HuggingFace-compatible) and RayDataset (distributed)\n",
                "   - Access nested fields with dot notation (e.g., `ds['meta.source']`)\n",
                "   - Apply operators via `.process()` method\n",
                "\n",
                "4. **[Chapter 5: Operators Usage](./05_Operators_Usage.ipynb)**\n",
                "   - Use operators programmatically via Python API\n",
                "   - Chain operators sequentially or batch process\n",
                "   - Inspect operator statistics\n",
                "\n",
                "5. **[Chapter 6: Analysis & Visualization](./06_Analysis_and_Visualization.ipynb)**\n",
                "   - Run data analysis with `dj-analyze`\n",
                "   - Interpret statistics and visualizations\n",
                "   - Compare datasets before and after processing\n",
                "\n",
                "6. **[Chapter 7: Distributed Processing with Ray](./07_Distributed_Processing_with_Ray.ipynb)**\n",
                "   - Set up Ray clusters (local and multi-node)\n",
                "   - Use demo configs from `demos/process_on_ray/`\n",
                "   - Monitor resources via Ray Dashboard\n",
                "   - Run distributed deduplication\n",
                "\n",
                "### Advanced Topics\n",
                "\n",
                "- **[Chapter 8: Pre-processing](./08_Preprocessing.ipynb)**\n",
                "  - Split datasets by language\n",
                "  - Convert raw formats (arXiv, Stack Exchange) to JSONL\n",
                "  - Serialize complex metadata fields\n",
                "\n",
                "- **[Chapter 9: Multimodal Data Processing](./09_Multimodal_Data_Processing.ipynb)**\n",
                "  - Understand multimodal format with special tokens\n",
                "  - Process image-text, video-text, audio-text data\n",
                "  - Convert between multimodal formats (LLaVA, Video-ChatGPT, WavCaps, etc.)\n",
                "  - Apply multimodal operators (image/video/audio filters)\n",
                "\n",
                "- **[Chapter 10: Advanced Dataset Configuration](./10_Advanced_Dataset_Configuration.ipynb)**\n",
                "  - Mix multiple datasets with custom weights\n",
                "  - Sample subsets from large datasets\n",
                "\n",
                "## Additional Resources\n",
                "\n",
                "- **Documentation**: https://datajuicer.github.io/data-juicer\n",
                "- **GitHub**: https://github.com/datajuicer/data-juicer\n",
                "- **Recipe Gallery**: https://datajuicer.github.io/data-juicer-hub\n",
                "- **Operator Reference**: https://datajuicer.github.io/data-juicer/en/main/docs/Operators.html"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-juicer-hub",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
